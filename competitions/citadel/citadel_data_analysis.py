#!/usr/bin/env python3
"""
Citadel Terminal AI Competition æ•°æ®åˆ†æè„šæœ¬
ä¸“é—¨ç”¨äºåˆ†æå’Œå‡†å¤‡ Citadel æ¯”èµ›çš„æ•°æ®ç»“æ„å’Œç‰¹å¾
"""

import sys
import json
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.ml.terminal_ai_tools import (
    RealTimeDataProcessor, 
    HighFrequencyStrategy, 
    AlgorithmOptimizer,
    PerformanceMonitor,
    create_terminal_ai_system,
    run_terminal_ai_simulation
)

class CitadelDataAnalyzer:
    """Citadel æ¯”èµ›æ•°æ®åˆ†æå™¨"""
    
    def __init__(self, config_path=None):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        if config_path is None:
            config_path = project_root / "configs" / "competitions" / "citadel_config.json"
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        self.data_dir = Path(__file__).parent / "data"
        self.features_dir = Path(__file__).parent / "features"
        self.models_dir = Path(__file__).parent / "models"
        
        # åˆ›å»ºç›®å½•
        for dir_path in [self.data_dir, self.features_dir, self.models_dir]:
            dir_path.mkdir(exist_ok=True)
    
    def generate_sample_data(self, n_samples=10000, n_assets=50):
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„é«˜é¢‘äº¤æ˜“æ•°æ®"""
        print(f"ğŸ“Š ç”Ÿæˆ {n_samples} æ¡æ ·æœ¬æ•°æ®ï¼Œ{n_assets} ä¸ªèµ„äº§...")
        
        # æ—¶é—´åºåˆ—
        start_time = datetime.now() - timedelta(days=30)
        timestamps = pd.date_range(start_time, periods=n_samples, freq='1min')
        
        data_list = []
        
        for asset_id in range(n_assets):
            # ç”Ÿæˆä»·æ ¼æ•°æ®
            np.random.seed(42 + asset_id)
            
            # åŸºç¡€ä»·æ ¼èµ°åŠ¿
            base_price = 100 + np.random.normal(0, 20)
            price_changes = np.random.normal(0, 0.001, n_samples)
            prices = base_price * np.exp(np.cumsum(price_changes))
            
            # OHLC æ•°æ®
            opens = prices * (1 + np.random.normal(0, 0.0005, n_samples))
            highs = np.maximum(opens, prices) * (1 + np.abs(np.random.normal(0, 0.001, n_samples)))
            lows = np.minimum(opens, prices) * (1 - np.abs(np.random.normal(0, 0.001, n_samples)))
            closes = prices
            
            # æˆäº¤é‡æ•°æ®
            base_volume = 1000 + np.random.exponential(500, n_samples)
            volumes = base_volume * (1 + np.random.normal(0, 0.5, n_samples))
            volumes = np.maximum(volumes, 100)  # æœ€å°æˆäº¤é‡
            
            # ä¹°å–ä»·å·®
            spreads = np.random.exponential(0.01, n_samples)
            bid_prices = closes - spreads / 2
            ask_prices = closes + spreads / 2
            
            # å¸‚åœºæ·±åº¦
            bid_sizes = np.random.exponential(1000, n_samples)
            ask_sizes = np.random.exponential(1000, n_samples)
            
            for i in range(n_samples):
                data_list.append({
                    'timestamp': timestamps[i],
                    'asset_id': f'ASSET_{asset_id:03d}',
                    'open': opens[i],
                    'high': highs[i],
                    'low': lows[i],
                    'close': closes[i],
                    'volume': volumes[i],
                    'bid_price': bid_prices[i],
                    'ask_price': ask_prices[i],
                    'bid_size': bid_sizes[i],
                    'ask_size': ask_sizes[i],
                    'spread': spreads[i]
                })
        
        df = pd.DataFrame(data_list)
        df = df.sort_values(['timestamp', 'asset_id']).reset_index(drop=True)
        
        # ä¿å­˜æ•°æ®
        data_file = self.data_dir / "sample_market_data.csv"
        df.to_csv(data_file, index=False)
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {data_file}")
        
        return df
    
    def analyze_data_structure(self, df):
        """åˆ†ææ•°æ®ç»“æ„"""
        print("\nğŸ“ˆ æ•°æ®ç»“æ„åˆ†æ:")
        print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"æ—¶é—´èŒƒå›´: {df['timestamp'].min()} åˆ° {df['timestamp'].max()}")
        print(f"èµ„äº§æ•°é‡: {df['asset_id'].nunique()}")
        print(f"æ•°æ®é¢‘ç‡: æ¯åˆ†é’Ÿ")
        
        print("\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'spread']
        print(df[numeric_cols].describe())
        
        print("\nğŸ” æ•°æ®è´¨é‡æ£€æŸ¥:")
        print(f"ç¼ºå¤±å€¼: {df.isnull().sum().sum()}")
        print(f"é‡å¤è¡Œ: {df.duplicated().sum()}")
        
        # ä»·æ ¼ä¸€è‡´æ€§æ£€æŸ¥
        price_issues = (df['high'] < df['low']) | (df['high'] < df['open']) | (df['high'] < df['close']) | \
                      (df['low'] > df['open']) | (df['low'] > df['close'])
        print(f"ä»·æ ¼ä¸ä¸€è‡´: {price_issues.sum()}")
        
        return {
            'shape': df.shape,
            'time_range': (df['timestamp'].min(), df['timestamp'].max()),
            'n_assets': df['asset_id'].nunique(),
            'missing_values': df.isnull().sum().sum(),
            'duplicates': df.duplicated().sum(),
            'price_issues': price_issues.sum()
        }
    
    def extract_features(self, df):
        """æå– Citadel æ¯”èµ›ç›¸å…³ç‰¹å¾"""
        print("\nğŸ”§ æå–é«˜é¢‘äº¤æ˜“ç‰¹å¾...")
        
        # åˆå§‹åŒ–å®æ—¶æ•°æ®å¤„ç†å™¨
        processor = RealTimeDataProcessor()
        
        feature_data = []
        
        for asset_id in df['asset_id'].unique():
            asset_data = df[df['asset_id'] == asset_id].copy()
            asset_data = asset_data.sort_values('timestamp').reset_index(drop=True)
            
            # åŸºç¡€ä»·æ ¼ç‰¹å¾
            asset_data['returns'] = asset_data['close'].pct_change()
            asset_data['log_returns'] = np.log(asset_data['close'] / asset_data['close'].shift(1))
            
            # æ³¢åŠ¨ç‡ç‰¹å¾
            asset_data['volatility_5min'] = asset_data['returns'].rolling(5).std()
            asset_data['volatility_15min'] = asset_data['returns'].rolling(15).std()
            asset_data['volatility_60min'] = asset_data['returns'].rolling(60).std()
            
            # åŠ¨é‡ç‰¹å¾
            asset_data['momentum_5min'] = asset_data['close'] / asset_data['close'].shift(5) - 1
            asset_data['momentum_15min'] = asset_data['close'] / asset_data['close'].shift(15) - 1
            asset_data['momentum_60min'] = asset_data['close'] / asset_data['close'].shift(60) - 1
            
            # æŠ€æœ¯æŒ‡æ ‡
            asset_data['rsi_14'] = processor._calculate_rsi(pd.Series(asset_data['close'].values), 14)
            asset_data['ma_5'] = asset_data['close'].rolling(5).mean()
            asset_data['ma_20'] = asset_data['close'].rolling(20).mean()
            asset_data['ma_ratio'] = asset_data['ma_5'] / asset_data['ma_20']
            
            # æˆäº¤é‡ç‰¹å¾
            asset_data['volume_ma_5'] = asset_data['volume'].rolling(5).mean()
            asset_data['volume_ratio'] = asset_data['volume'] / asset_data['volume_ma_5']
            asset_data['vwap'] = (asset_data['close'] * asset_data['volume']).rolling(20).sum() / asset_data['volume'].rolling(20).sum()
            asset_data['price_vwap_ratio'] = asset_data['close'] / asset_data['vwap']
            
            # ä¹°å–ä»·å·®ç‰¹å¾
            asset_data['spread_pct'] = asset_data['spread'] / asset_data['close']
            asset_data['spread_ma'] = asset_data['spread_pct'].rolling(10).mean()
            asset_data['spread_volatility'] = asset_data['spread_pct'].rolling(10).std()
            
            # å¸‚åœºå¾®è§‚ç»“æ„ç‰¹å¾
            asset_data['bid_ask_imbalance'] = (asset_data['bid_size'] - asset_data['ask_size']) / (asset_data['bid_size'] + asset_data['ask_size'])
            asset_data['mid_price'] = (asset_data['bid_price'] + asset_data['ask_price']) / 2
            asset_data['price_impact'] = (asset_data['close'] - asset_data['mid_price']) / asset_data['spread']
            
            # é«˜é¢‘ç‰¹å¾
            asset_data['price_acceleration'] = asset_data['returns'].diff()
            asset_data['volume_acceleration'] = asset_data['volume'].pct_change().diff()
            
            # è·¨æ—¶é—´æ¡†æ¶ç‰¹å¾
            asset_data['intraday_return'] = asset_data.groupby(asset_data['timestamp'].dt.date)['returns'].cumsum()
            asset_data['time_of_day'] = asset_data['timestamp'].dt.hour * 60 + asset_data['timestamp'].dt.minute
            asset_data['day_of_week'] = asset_data['timestamp'].dt.dayofweek
            
            feature_data.append(asset_data)
        
        # åˆå¹¶æ‰€æœ‰èµ„äº§çš„ç‰¹å¾æ•°æ®
        features_df = pd.concat(feature_data, ignore_index=True)
        
        # ä¿å­˜ç‰¹å¾æ•°æ®
        features_file = self.features_dir / "citadel_features.csv"
        features_df.to_csv(features_file, index=False)
        print(f"âœ… ç‰¹å¾æ•°æ®å·²ä¿å­˜åˆ°: {features_file}")
        
        # ç‰¹å¾ç»Ÿè®¡
        feature_cols = [col for col in features_df.columns if col not in ['timestamp', 'asset_id']]
        print(f"\nğŸ“Š æå–äº† {len(feature_cols)} ä¸ªç‰¹å¾:")
        for i, col in enumerate(feature_cols, 1):
            print(f"{i:2d}. {col}")
        
        return features_df
    
    def analyze_trading_patterns(self, df):
        """åˆ†æäº¤æ˜“æ¨¡å¼"""
        print("\nğŸ“ˆ äº¤æ˜“æ¨¡å¼åˆ†æ:")
        
        # æ—¶é—´æ¨¡å¼
        df['hour'] = df['timestamp'].dt.hour
        df['minute'] = df['timestamp'].dt.minute
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        
        # æˆäº¤é‡æ¨¡å¼
        hourly_volume = df.groupby('hour')['volume'].mean()
        print(f"æˆäº¤é‡æœ€é«˜æ—¶æ®µ: {hourly_volume.idxmax()}:00 (å¹³å‡æˆäº¤é‡: {hourly_volume.max():.0f})")
        print(f"æˆäº¤é‡æœ€ä½æ—¶æ®µ: {hourly_volume.idxmin()}:00 (å¹³å‡æˆäº¤é‡: {hourly_volume.min():.0f})")
        
        # æ³¢åŠ¨ç‡æ¨¡å¼
        df['returns'] = df.groupby('asset_id')['close'].pct_change()
        hourly_volatility = df.groupby('hour')['returns'].std()
        print(f"æ³¢åŠ¨ç‡æœ€é«˜æ—¶æ®µ: {hourly_volatility.idxmax()}:00 (æ ‡å‡†å·®: {hourly_volatility.max():.4f})")
        print(f"æ³¢åŠ¨ç‡æœ€ä½æ—¶æ®µ: {hourly_volatility.idxmin()}:00 (æ ‡å‡†å·®: {hourly_volatility.min():.4f})")
        
        # ä»·å·®æ¨¡å¼
        hourly_spread = df.groupby('hour')['spread'].mean()
        print(f"ä»·å·®æœ€å¤§æ—¶æ®µ: {hourly_spread.idxmax()}:00 (å¹³å‡ä»·å·®: {hourly_spread.max():.4f})")
        print(f"ä»·å·®æœ€å°æ—¶æ®µ: {hourly_spread.idxmin()}:00 (å¹³å‡ä»·å·®: {hourly_spread.min():.4f})")
        
        return {
            'volume_patterns': hourly_volume.to_dict(),
            'volatility_patterns': hourly_volatility.to_dict(),
            'spread_patterns': hourly_spread.to_dict()
        }
    
    def run_strategy_simulation(self, features_df):
        """è¿è¡Œç­–ç•¥æ¨¡æ‹Ÿ"""
        print("\nğŸš€ è¿è¡Œ Citadel ç­–ç•¥æ¨¡æ‹Ÿ...")
        
        # é€‰æ‹©ä¸€ä¸ªèµ„äº§è¿›è¡Œæ¨¡æ‹Ÿ
        sample_asset = features_df[features_df['asset_id'] == 'ASSET_000'].copy()
        sample_asset = sample_asset.dropna().reset_index(drop=True)
        
        if len(sample_asset) < 100:
            print("âš ï¸ æ•°æ®ä¸è¶³ï¼Œè·³è¿‡ç­–ç•¥æ¨¡æ‹Ÿ")
            return None
        
        # å‡†å¤‡æ•°æ®
        price_data = sample_asset[['timestamp', 'open', 'high', 'low', 'close', 'volume']].copy()
        price_data.set_index('timestamp', inplace=True)
        
        # è¿è¡Œ Terminal AI ç³»ç»Ÿæ¨¡æ‹Ÿ
        try:
            results = run_terminal_ai_simulation(
                price_data=price_data,
                initial_capital=100000,
                lookback_period=60,
                rebalance_freq='5min'
            )
            
            print("âœ… ç­–ç•¥æ¨¡æ‹Ÿå®Œæˆ!")
            print(f"æ€»æ”¶ç›Šç‡: {results['total_return']:.2%}")
            print(f"å¤æ™®æ¯”ç‡: {results['sharpe_ratio']:.3f}")
            print(f"æœ€å¤§å›æ’¤: {results['max_drawdown']:.2%}")
            print(f"èƒœç‡: {results['win_rate']:.2%}")
            
            # ä¿å­˜ç»“æœ
            results_file = self.models_dir / "simulation_results.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            
            return results
            
        except Exception as e:
            print(f"âš ï¸ ç­–ç•¥æ¨¡æ‹Ÿå¤±è´¥: {e}")
            return None
    
    def generate_analysis_report(self, data_stats, trading_patterns, simulation_results=None):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        report = {
            "competition": "Citadel Terminal AI Competition",
            "analysis_date": datetime.now().isoformat(),
            "data_analysis": {
                "data_shape": data_stats['shape'],
                "time_range": [str(data_stats['time_range'][0]), str(data_stats['time_range'][1])],
                "n_assets": data_stats['n_assets'],
                "data_quality": {
                    "missing_values": data_stats['missing_values'],
                    "duplicates": data_stats['duplicates'],
                    "price_issues": data_stats['price_issues']
                }
            },
            "trading_patterns": trading_patterns,
            "key_insights": [
                "é«˜é¢‘æ•°æ®éœ€è¦é‡ç‚¹å…³æ³¨å¸‚åœºå¾®è§‚ç»“æ„ç‰¹å¾",
                "ä¹°å–ä»·å·®å’Œè®¢å•ç°¿ä¸å¹³è¡¡æ˜¯é‡è¦çš„é¢„æµ‹å› å­",
                "æ—¶é—´ç‰¹å¾ï¼ˆå°æ—¶ã€åˆ†é’Ÿï¼‰å¯¹ç­–ç•¥è¡¨ç°æœ‰æ˜¾è‘—å½±å“",
                "æˆäº¤é‡æ¨¡å¼å¯ä»¥å¸®åŠ©è¯†åˆ«æœ€ä½³äº¤æ˜“æ—¶æœº",
                "å®æ—¶é£é™©ç®¡ç†å¯¹é«˜é¢‘ç­–ç•¥è‡³å…³é‡è¦"
            ],
            "recommended_features": [
                "ä»·æ ¼åŠ¨é‡ï¼ˆå¤šæ—¶é—´æ¡†æ¶ï¼‰",
                "æ³¢åŠ¨ç‡ç‰¹å¾ï¼ˆå®ç°æ³¢åŠ¨ç‡ã€GARCHï¼‰",
                "æˆäº¤é‡ç‰¹å¾ï¼ˆVWAPã€æˆäº¤é‡æ¯”ç‡ï¼‰",
                "å¸‚åœºå¾®è§‚ç»“æ„ï¼ˆä¹°å–ä»·å·®ã€è®¢å•ä¸å¹³è¡¡ï¼‰",
                "æŠ€æœ¯æŒ‡æ ‡ï¼ˆRSIã€ç§»åŠ¨å¹³å‡ã€å¸ƒæ—å¸¦ï¼‰",
                "æ—¶é—´ç‰¹å¾ï¼ˆæ—¥å†…æ—¶é—´ã€æ˜ŸæœŸå‡ ï¼‰",
                "è·¨èµ„äº§ç‰¹å¾ï¼ˆç›¸å…³æ€§ã€åæ•´ï¼‰"
            ],
            "strategy_recommendations": [
                "å¤šç­–ç•¥ç»„åˆï¼šåŠ¨é‡ + å‡å€¼å›å½’ + å¥—åˆ©",
                "å®æ—¶å‚æ•°ä¼˜åŒ–å’Œç­–ç•¥åˆ‡æ¢",
                "ä¸¥æ ¼çš„é£é™©æ§åˆ¶å’Œä»“ä½ç®¡ç†",
                "ä½å»¶è¿Ÿæ‰§è¡Œå’Œè®¢å•ä¼˜åŒ–",
                "æœºå™¨å­¦ä¹ æ¨¡å‹çš„åœ¨çº¿å­¦ä¹ "
            ]
        }
        
        if simulation_results:
            report["simulation_results"] = simulation_results
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = Path(__file__).parent / "citadel_analysis_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Citadel Terminal AI Competition æ•°æ®åˆ†æ")
    print("=" * 50)
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = CitadelDataAnalyzer()
    
    # ç”Ÿæˆæ ·æœ¬æ•°æ®
    df = analyzer.generate_sample_data(n_samples=5000, n_assets=10)
    
    # åˆ†ææ•°æ®ç»“æ„
    data_stats = analyzer.analyze_data_structure(df)
    
    # æå–ç‰¹å¾
    features_df = analyzer.extract_features(df)
    
    # åˆ†æäº¤æ˜“æ¨¡å¼
    trading_patterns = analyzer.analyze_trading_patterns(df)
    
    # è¿è¡Œç­–ç•¥æ¨¡æ‹Ÿ
    simulation_results = analyzer.run_strategy_simulation(features_df)
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    report = analyzer.generate_analysis_report(data_stats, trading_patterns, simulation_results)
    
    print("\nğŸ‰ Citadel æ•°æ®åˆ†æå®Œæˆ!")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {Path(__file__).parent}")
    print("\nğŸ“‹ å…³é”®å‘ç°:")
    for insight in report['key_insights']:
        print(f"  â€¢ {insight}")
    
    print("\nğŸš€ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("  1. è·å–çœŸå®çš„é«˜é¢‘å¸‚åœºæ•°æ®")
    print("  2. å®æ–½æ¨èçš„ç‰¹å¾å·¥ç¨‹ç­–ç•¥")
    print("  3. å¼€å‘å’Œæµ‹è¯•å¤šç­–ç•¥ç»„åˆ")
    print("  4. ä¼˜åŒ–å®æ—¶æ‰§è¡Œç³»ç»Ÿ")
    print("  5. è¿›è¡Œå……åˆ†çš„å›æµ‹å’Œé£é™©è¯„ä¼°")

if __name__ == "__main__":
    main()