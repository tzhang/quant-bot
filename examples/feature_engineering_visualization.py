#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§ç‰¹å¾å·¥ç¨‹å¯è§†åŒ–æ¼”ç¤º

å±•ç¤ºç‰¹å¾å·¥ç¨‹å‰åçš„æ•ˆæœå¯¹æ¯”å’Œç‰¹å¾è´¨é‡åˆ†æ
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
import warnings
warnings.filterwarnings('ignore')

from src.ml.advanced_feature_engineering import AdvancedFeatureEngineeringPipeline
from src.ml.feature_selection import ImportanceBasedSelection

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False


def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ® - ä»…ç”¨äºæµ‹è¯•å’Œæ¼”ç¤º"""
    np.random.seed(42)
    
    # ç”Ÿæˆ2å¹´çš„æ—¥åº¦æ•°æ® - æ¨¡æ‹Ÿæ•°æ®ä»…ç”¨äºæ¼”ç¤º
    dates = pd.date_range('2022-01-01', periods=500, freq='D')
    n = len(dates)
    
    # ç”Ÿæˆä»·æ ¼åºåˆ—ï¼ˆéšæœºæ¸¸èµ° + è¶‹åŠ¿ï¼‰
    trend = np.linspace(0, 0.5, n)
    returns = np.random.normal(0.001, 0.02, n) + trend * 0.0001
    price = 100 * np.exp(np.cumsum(returns))
    
    # ç”Ÿæˆæˆäº¤é‡ï¼ˆå¯¹æ•°æ­£æ€åˆ†å¸ƒ + å‘¨æœŸæ€§ï¼‰
    volume_base = np.random.lognormal(15, 0.3, n)
    volume_cycle = 0.2 * np.sin(2 * np.pi * np.arange(n) / 20)  # 20å¤©å‘¨æœŸ
    volume = volume_base * (1 + volume_cycle)
    
    # ç”Ÿæˆæ³¢åŠ¨ç‡ï¼ˆGARCHç±»ä¼¼ï¼‰
    volatility = np.abs(returns) * 100
    for i in range(1, n):
        volatility[i] = 0.1 * volatility[i] + 0.9 * volatility[i-1]
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame({
        'date': dates,
        'price': price,
        'returns': returns,
        'volume': volume,
        'volatility': volatility
    })
    
    return df


def visualize_original_vs_engineered():
    """å¯è§†åŒ–åŸå§‹ç‰¹å¾ vs å·¥ç¨‹ç‰¹å¾"""
    print("ç”Ÿæˆç‰¹å¾å·¥ç¨‹å¯¹æ¯”å›¾...")
    
    # åˆ›å»ºæ•°æ®
    df = create_sample_data()
    
    # åº”ç”¨ç‰¹å¾å·¥ç¨‹
    pipeline = AdvancedFeatureEngineeringPipeline()
    df_enhanced = pipeline.transform(
        df,
        feature_types=['stats', 'frequency'],
        columns=['returns', 'volume', 'volatility']
    )
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('ç‰¹å¾å·¥ç¨‹å‰åå¯¹æ¯”', fontsize=16, fontweight='bold')
    
    # åŸå§‹ç‰¹å¾åˆ†å¸ƒ
    axes[0, 0].hist(df['returns'], bins=30, alpha=0.7, color='blue', edgecolor='black')
    axes[0, 0].set_title('åŸå§‹æ”¶ç›Šç‡åˆ†å¸ƒ')
    axes[0, 0].set_xlabel('æ”¶ç›Šç‡')
    axes[0, 0].set_ylabel('é¢‘æ¬¡')
    
    # å·¥ç¨‹ç‰¹å¾åˆ†å¸ƒï¼ˆé€‰æ‹©ä¸€ä¸ªç»Ÿè®¡ç‰¹å¾ï¼‰
    stat_features = [col for col in df_enhanced.columns if 'skewness' in col]
    if stat_features:
        axes[0, 1].hist(df_enhanced[stat_features[0]].dropna(), bins=30, alpha=0.7, color='red', edgecolor='black')
        axes[0, 1].set_title(f'å·¥ç¨‹ç‰¹å¾åˆ†å¸ƒ\n({stat_features[0]})')
        axes[0, 1].set_xlabel('ç‰¹å¾å€¼')
        axes[0, 1].set_ylabel('é¢‘æ¬¡')
    
    # åŸå§‹ç‰¹å¾æ—¶é—´åºåˆ—
    axes[1, 0].plot(df['date'], df['returns'], alpha=0.7, color='blue')
    axes[1, 0].set_title('åŸå§‹æ”¶ç›Šç‡æ—¶é—´åºåˆ—')
    axes[1, 0].set_xlabel('æ—¥æœŸ')
    axes[1, 0].set_ylabel('æ”¶ç›Šç‡')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    # å·¥ç¨‹ç‰¹å¾æ—¶é—´åºåˆ—
    if stat_features:
        valid_data = df_enhanced[stat_features[0]].dropna()
        valid_dates = df_enhanced.loc[valid_data.index, 'date']
        axes[1, 1].plot(valid_dates, valid_data, alpha=0.7, color='red')
        axes[1, 1].set_title(f'å·¥ç¨‹ç‰¹å¾æ—¶é—´åºåˆ—\n({stat_features[0]})')
        axes[1, 1].set_xlabel('æ—¥æœŸ')
        axes[1, 1].set_ylabel('ç‰¹å¾å€¼')
        axes[1, 1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig('feature_engineering_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return df, df_enhanced


def visualize_feature_importance():
    """å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§"""
    print("ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾...")
    
    # åˆ›å»ºæ•°æ®
    df = create_sample_data()
    
    # åº”ç”¨ç‰¹å¾å·¥ç¨‹
    pipeline = AdvancedFeatureEngineeringPipeline()
    df_enhanced = pipeline.transform(
        df,
        feature_types=['stats', 'frequency'],
        columns=['returns', 'volume', 'volatility']
    )
    
    # åˆ›å»ºç›®æ ‡å˜é‡
    df_enhanced['target'] = df_enhanced['returns'].shift(-1)
    df_enhanced = df_enhanced.dropna()
    
    # ç‰¹å¾é€‰æ‹©
    feature_cols = [col for col in df_enhanced.columns if col not in ['date', 'target']]
    X = df_enhanced[feature_cols]
    y = df_enhanced['target']
    
    # æ¸…ç†æ•°æ®ï¼šå¤„ç†æ— ç©·å¤§å€¼å’ŒNaNå€¼
    print("æ¸…ç†ç‰¹å¾å·¥ç¨‹åçš„æ•°æ®...")
    X = X.replace([np.inf, -np.inf], np.nan)
    X = X.fillna(X.median())
    
    # æ ‡å‡†åŒ–ç‰¹å¾ä»¥é¿å…æ•°å€¼è¿‡å¤§
    scaler = StandardScaler()
    X_scaled = pd.DataFrame(
        scaler.fit_transform(X),
        columns=X.columns,
        index=X.index
    )
    
    # è®¡ç®—ç‰¹å¾é‡è¦æ€§
    selector = ImportanceBasedSelection(
        method='random_forest',
        threshold='median',
        n_estimators=100
    )
    selector.fit(X_scaled, y)
    
    # è·å–é‡è¦æ€§åˆ†æ•°ï¼ˆä½¿ç”¨feature_importance_å±æ€§ï¼‰
    importance_df = pd.DataFrame({
        'feature': selector.feature_importance_.index,
        'importance': selector.feature_importance_.values
    }).sort_values('importance', ascending=False)
    
    # ç»˜åˆ¶å‰20ä¸ªæœ€é‡è¦çš„ç‰¹å¾
    plt.figure(figsize=(12, 8))
    top_features = importance_df.head(20)
    
    bars = plt.barh(range(len(top_features)), top_features['importance'])
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('ç‰¹å¾é‡è¦æ€§')
    plt.title('Top 20 ç‰¹å¾é‡è¦æ€§æ’å', fontsize=14, fontweight='bold')
    plt.gca().invert_yaxis()
    
    # ä¸ºä¸åŒç±»å‹çš„ç‰¹å¾è®¾ç½®ä¸åŒé¢œè‰²
    colors = []
    for feature in top_features['feature']:
        if any(x in feature for x in ['moment', 'skewness', 'kurtosis', 'entropy']):
            colors.append('skyblue')  # ç»Ÿè®¡ç‰¹å¾
        elif any(x in feature for x in ['fft', 'spectral', 'dominant']):
            colors.append('lightcoral')  # é¢‘åŸŸç‰¹å¾
        else:
            colors.append('lightgreen')  # åŸå§‹ç‰¹å¾
    
    for bar, color in zip(bars, colors):
        bar.set_color(color)
    
    # æ·»åŠ å›¾ä¾‹
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='skyblue', label='ç»Ÿè®¡ç‰¹å¾'),
        Patch(facecolor='lightcoral', label='é¢‘åŸŸç‰¹å¾'),
        Patch(facecolor='lightgreen', label='åŸå§‹ç‰¹å¾')
    ]
    plt.legend(handles=legend_elements, loc='lower right')
    
    plt.tight_layout()
    plt.savefig('feature_importance_ranking.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return importance_df


def visualize_feature_correlation():
    """å¯è§†åŒ–ç‰¹å¾ç›¸å…³æ€§"""
    print("ç”Ÿæˆç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾...")
    
    # åˆ›å»ºæ•°æ®
    df = create_sample_data()
    
    # åº”ç”¨ç‰¹å¾å·¥ç¨‹
    pipeline = AdvancedFeatureEngineeringPipeline()
    df_enhanced = pipeline.transform(
        df,
        feature_types=['stats'],
        columns=['returns', 'volume', 'volatility']
    )
    
    # é€‰æ‹©ä¸€äº›ä»£è¡¨æ€§ç‰¹å¾
    original_features = ['returns', 'volume', 'volatility']
    stat_features = [col for col in df_enhanced.columns if any(x in col for x in ['skewness', 'kurtosis', 'moment'])][:6]
    selected_features = original_features + stat_features
    
    # æ¸…ç†æ•°æ®ï¼šå¤„ç†æ— ç©·å¤§å€¼å’ŒNaNå€¼
    df_enhanced[selected_features] = df_enhanced[selected_features].replace([np.inf, -np.inf], np.nan)
    df_enhanced[selected_features] = df_enhanced[selected_features].fillna(df_enhanced[selected_features].median())
    
    # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
    correlation_matrix = df_enhanced[selected_features].corr()
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    plt.figure(figsize=(10, 8))
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    
    sns.heatmap(correlation_matrix, 
                mask=mask,
                annot=True, 
                cmap='RdBu_r', 
                center=0,
                square=True,
                fmt='.2f',
                cbar_kws={'shrink': 0.8})
    
    plt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig('feature_correlation_heatmap.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return correlation_matrix


def visualize_pca_analysis():
    """å¯è§†åŒ–PCAåˆ†æ"""
    print("ç”ŸæˆPCAåˆ†æå›¾...")
    
    # åˆ›å»ºæ•°æ®
    df = create_sample_data()
    
    # åº”ç”¨ç‰¹å¾å·¥ç¨‹
    pipeline = AdvancedFeatureEngineeringPipeline()
    df_enhanced = pipeline.transform(
        df,
        feature_types=['stats', 'frequency'],
        columns=['returns', 'volume', 'volatility']
    )
    
    # å‡†å¤‡æ•°æ®
    feature_cols = [col for col in df_enhanced.columns if col not in ['date']]
    X = df_enhanced[feature_cols].dropna()
    
    # æ¸…ç†æ•°æ®ï¼šå¤„ç†æ— ç©·å¤§å€¼å’ŒNaNå€¼
    X = X.replace([np.inf, -np.inf], np.nan)
    X = X.fillna(X.median())
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # PCAåˆ†æ
    pca = PCA()
    X_pca = pca.fit_transform(X_scaled)
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # è§£é‡Šæ–¹å·®æ¯”ä¾‹
    cumsum_ratio = np.cumsum(pca.explained_variance_ratio_)
    axes[0].plot(range(1, min(21, len(cumsum_ratio)+1)), cumsum_ratio[:20], 'bo-')
    axes[0].axhline(y=0.95, color='r', linestyle='--', label='95%è§£é‡Šæ–¹å·®')
    axes[0].set_xlabel('ä¸»æˆåˆ†æ•°é‡')
    axes[0].set_ylabel('ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”ä¾‹')
    axes[0].set_title('PCAç´¯ç§¯è§£é‡Šæ–¹å·®')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # å‰ä¸¤ä¸ªä¸»æˆåˆ†çš„æ•£ç‚¹å›¾
    axes[1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6, c=range(len(X_pca)), cmap='viridis')
    axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} è§£é‡Šæ–¹å·®)')
    axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} è§£é‡Šæ–¹å·®)')
    axes[1].set_title('å‰ä¸¤ä¸ªä¸»æˆåˆ†æ•£ç‚¹å›¾')
    
    plt.tight_layout()
    plt.savefig('pca_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return pca


def visualize_model_performance():
    """å¯è§†åŒ–æ¨¡å‹æ€§èƒ½å¯¹æ¯”"""
    print("ç”Ÿæˆæ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾...")
    
    # åˆ›å»ºæ•°æ®
    df = create_sample_data()
    
    # åŸå§‹ç‰¹å¾
    original_features = ['returns', 'volume', 'volatility']
    df['target'] = df['returns'].shift(-1)
    df_clean = df.dropna()
    
    X_original = df_clean[original_features]
    y = df_clean['target']
    
    # å·¥ç¨‹ç‰¹å¾
    pipeline = AdvancedFeatureEngineeringPipeline()
    df_enhanced = pipeline.transform(
        df,
        feature_types=['stats', 'frequency'],
        columns=['returns', 'volume', 'volatility']
    )
    df_enhanced['target'] = df_enhanced['returns'].shift(-1)
    df_enhanced_clean = df_enhanced.dropna()
    
    feature_cols = [col for col in df_enhanced_clean.columns if col not in ['date', 'target']]
    X_enhanced = df_enhanced_clean[feature_cols]
    y_enhanced = df_enhanced_clean['target']
    
    # æ¸…ç†æ•°æ®ï¼šå¤„ç†æ— ç©·å¤§å€¼å’ŒNaNå€¼
    X_enhanced = X_enhanced.replace([np.inf, -np.inf], np.nan)
    X_enhanced = X_enhanced.fillna(X_enhanced.median())
    
    # æ¨¡å‹æ€§èƒ½å¯¹æ¯”
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    
    # åŸå§‹ç‰¹å¾æ€§èƒ½
    scores_original = cross_val_score(model, X_original, y, cv=5, scoring='neg_mean_squared_error')
    mse_original = -scores_original.mean()
    
    # å·¥ç¨‹ç‰¹å¾æ€§èƒ½
    scores_enhanced = cross_val_score(model, X_enhanced, y_enhanced, cv=5, scoring='neg_mean_squared_error')
    mse_enhanced = -scores_enhanced.mean()
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾
    plt.figure(figsize=(10, 6))
    
    categories = ['åŸå§‹ç‰¹å¾', 'å·¥ç¨‹ç‰¹å¾']
    mse_values = [mse_original, mse_enhanced]
    improvement = (mse_original - mse_enhanced) / mse_original * 100
    
    bars = plt.bar(categories, mse_values, color=['lightblue', 'lightcoral'], alpha=0.8, edgecolor='black')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, mse in zip(bars, mse_values):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + mse*0.01, 
                f'{mse:.6f}', ha='center', va='bottom', fontweight='bold')
    
    plt.ylabel('å‡æ–¹è¯¯å·® (MSE)')
    plt.title(f'æ¨¡å‹æ€§èƒ½å¯¹æ¯”\n(ç‰¹å¾å·¥ç¨‹æ”¹å–„: {improvement:.1f}%)', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ æ”¹å–„ç™¾åˆ†æ¯”æ³¨é‡Š
    plt.annotate(f'æ”¹å–„ {improvement:.1f}%', 
                xy=(1, mse_enhanced), xytext=(0.5, mse_enhanced * 0.5),
                arrowprops=dict(arrowstyle='->', color='red', lw=2),
                fontsize=12, fontweight='bold', color='red',
                ha='center')
    
    plt.tight_layout()
    plt.savefig('model_performance_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return {
        'original_mse': mse_original,
        'enhanced_mse': mse_enhanced,
        'improvement': improvement
    }


def main():
    """ä¸»å‡½æ•°"""
    print("é«˜çº§ç‰¹å¾å·¥ç¨‹å¯è§†åŒ–æ¼”ç¤º")
    print("="*60)
    
    try:
        # 1. ç‰¹å¾å·¥ç¨‹å‰åå¯¹æ¯”
        print("\n1. ç”Ÿæˆç‰¹å¾å·¥ç¨‹å‰åå¯¹æ¯”å›¾...")
        df_original, df_enhanced = visualize_original_vs_engineered()
        print(f"   åŸå§‹ç‰¹å¾æ•°: {df_original.shape[1]}")
        print(f"   å·¥ç¨‹åç‰¹å¾æ•°: {df_enhanced.shape[1]}")
        
        # 2. ç‰¹å¾é‡è¦æ€§åˆ†æ
        print("\n2. ç”Ÿæˆç‰¹å¾é‡è¦æ€§æ’åå›¾...")
        importance_df = visualize_feature_importance()
        print(f"   åˆ†æäº† {len(importance_df)} ä¸ªç‰¹å¾çš„é‡è¦æ€§")
        
        # 3. ç‰¹å¾ç›¸å…³æ€§åˆ†æ
        print("\n3. ç”Ÿæˆç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾...")
        correlation_matrix = visualize_feature_correlation()
        print(f"   åˆ†æäº† {correlation_matrix.shape[0]} ä¸ªç‰¹å¾çš„ç›¸å…³æ€§")
        
        # 4. PCAåˆ†æ
        print("\n4. ç”ŸæˆPCAåˆ†æå›¾...")
        pca = visualize_pca_analysis()
        print(f"   å‰5ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[:5].sum():.1%}")
        
        # 5. æ¨¡å‹æ€§èƒ½å¯¹æ¯”
        print("\n5. ç”Ÿæˆæ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾...")
        performance = visualize_model_performance()
        print(f"   æ¨¡å‹æ€§èƒ½æ”¹å–„: {performance['improvement']:.1f}%")
        
        print("\n" + "="*60)
        print("å¯è§†åŒ–æ¼”ç¤ºå®Œæˆï¼")
        print("="*60)
        
        print("\nç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶:")
        print("ğŸ“Š feature_engineering_comparison.png - ç‰¹å¾å·¥ç¨‹å‰åå¯¹æ¯”")
        print("ğŸ“Š feature_importance_ranking.png - ç‰¹å¾é‡è¦æ€§æ’å")
        print("ğŸ“Š feature_correlation_heatmap.png - ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾")
        print("ğŸ“Š pca_analysis.png - PCAåˆ†æ")
        print("ğŸ“Š model_performance_comparison.png - æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
        
        print(f"\næ€»ç»“:")
        print(f"âœ… ç‰¹å¾æ•°é‡ä» {df_original.shape[1]} å¢åŠ åˆ° {df_enhanced.shape[1]}")
        print(f"âœ… æ¨¡å‹æ€§èƒ½æ”¹å–„ {performance['improvement']:.1f}%")
        print(f"âœ… å‰5ä¸ªä¸»æˆåˆ†è§£é‡Š {pca.explained_variance_ratio_[:5].sum():.1%} çš„æ–¹å·®")
        
    except Exception as e:
        print(f"å¯è§†åŒ–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()