#!/usr/bin/env python3
"""
å†…å­˜ä¼˜åŒ–æµ‹è¯•
ç›‘æ§å¤§æ•°æ®é›†å¤„ç†æ—¶çš„å†…å­˜ä½¿ç”¨æƒ…å†µå¹¶æä¾›ä¼˜åŒ–å»ºè®®
"""

import sys
import os
import time
import gc
import psutil
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple
import tracemalloc

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from backtesting.enhanced_backtest_engine import (
    EnhancedBacktestEngine, 
    ParallelBacktestEngine,
    SimpleMovingAverageStrategy
)

class MemoryMonitor:
    """å†…å­˜ç›‘æ§å™¨"""
    
    def __init__(self):
        self.process = psutil.Process()
        self.memory_snapshots = []
        self.tracemalloc_snapshots = []
        
    def start_monitoring(self):
        """å¼€å§‹å†…å­˜ç›‘æ§"""
        tracemalloc.start()
        self.memory_snapshots = []
        self.tracemalloc_snapshots = []
        
    def take_snapshot(self, label: str):
        """æ‹æ‘„å†…å­˜å¿«ç…§"""
        # ç³»ç»Ÿå†…å­˜
        memory_info = self.process.memory_info()
        memory_percent = self.process.memory_percent()
        
        # Pythonå†…å­˜
        snapshot = tracemalloc.take_snapshot()
        
        self.memory_snapshots.append({
            'label': label,
            'timestamp': time.time(),
            'rss_mb': memory_info.rss / 1024 / 1024,  # ç‰©ç†å†…å­˜
            'vms_mb': memory_info.vms / 1024 / 1024,  # è™šæ‹Ÿå†…å­˜
            'percent': memory_percent,
            'tracemalloc_size_mb': sum(stat.size for stat in snapshot.statistics('lineno')) / 1024 / 1024
        })
        
        self.tracemalloc_snapshots.append((label, snapshot))
        
    def get_memory_usage(self) -> Dict:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory_info = self.process.memory_info()
        return {
            'rss_mb': memory_info.rss / 1024 / 1024,
            'vms_mb': memory_info.vms / 1024 / 1024,
            'percent': self.process.memory_percent()
        }
        
    def analyze_memory_growth(self) -> Dict:
        """åˆ†æå†…å­˜å¢é•¿"""
        if len(self.memory_snapshots) < 2:
            return {}
            
        first = self.memory_snapshots[0]
        last = self.memory_snapshots[-1]
        
        return {
            'initial_rss_mb': first['rss_mb'],
            'final_rss_mb': last['rss_mb'],
            'growth_rss_mb': last['rss_mb'] - first['rss_mb'],
            'growth_percent': ((last['rss_mb'] - first['rss_mb']) / first['rss_mb']) * 100,
            'peak_rss_mb': max(s['rss_mb'] for s in self.memory_snapshots),
            'snapshots': self.memory_snapshots
        }
        
    def get_top_memory_consumers(self, limit: int = 10) -> List[Dict]:
        """è·å–å†…å­˜æ¶ˆè€—æœ€å¤§çš„ä»£ç ä½ç½®"""
        if not self.tracemalloc_snapshots:
            return []
            
        latest_snapshot = self.tracemalloc_snapshots[-1][1]
        top_stats = latest_snapshot.statistics('lineno')[:limit]
        
        consumers = []
        for stat in top_stats:
            consumers.append({
                'filename': stat.traceback.format()[-1] if stat.traceback.format() else 'unknown',
                'size_mb': stat.size / 1024 / 1024,
                'count': stat.count
            })
            
        return consumers

def create_memory_efficient_data(num_symbols: int, days: int, chunk_size: int = 10) -> Dict[str, pd.DataFrame]:
    """åˆ›å»ºå†…å­˜é«˜æ•ˆçš„æµ‹è¯•æ•°æ® - ä»…ç”¨äºæµ‹è¯•å’Œæ¼”ç¤º"""
    print(f"åˆ›å»º {num_symbols} ä¸ªè‚¡ç¥¨ {days} å¤©çš„æ•°æ® (åˆ†å—å¤„ç†)...")
    
    market_data = {}
    start_date = datetime(2023, 1, 1)
    
    # åˆ†å—å¤„ç†ä»¥å‡å°‘å†…å­˜å³°å€¼
    for chunk_start in range(0, num_symbols, chunk_size):
        chunk_end = min(chunk_start + chunk_size, num_symbols)
        
        for i in range(chunk_start, chunk_end):
            symbol = f"STOCK_{i:03d}"
            dates = pd.date_range(start=start_date, periods=days, freq='D')
            
            # ä½¿ç”¨æ›´èŠ‚çœå†…å­˜çš„æ•°æ®ç±»å‹ - æ¨¡æ‹Ÿæ•°æ®ä»…ç”¨äºæ¼”ç¤º
            np.random.seed(i)
            returns = np.random.normal(0.001, 0.02, days).astype(np.float32)  # æ¨¡æ‹Ÿæ”¶ç›Šç‡ä»…ç”¨äºæµ‹è¯•
            prices = np.zeros(days, dtype=np.float32)
            prices[0] = 100.0
            
            for j in range(1, days):
                prices[j] = prices[j-1] * (1 + returns[j])
            
            # ä½¿ç”¨é€‚å½“çš„æ•°æ®ç±»å‹
            market_data[symbol] = pd.DataFrame({
                'open': prices,
                'high': (prices * (1 + np.random.uniform(0, 0.02, days))).astype(np.float32),  # æ¨¡æ‹Ÿé«˜ä»·ä»…ç”¨äºæµ‹è¯•
                'low': (prices * (1 - np.random.uniform(0, 0.02, days))).astype(np.float32),   # æ¨¡æ‹Ÿä½ä»·ä»…ç”¨äºæµ‹è¯•
                'close': prices,
                'volume': np.random.randint(10000, 100000, days, dtype=np.int32)  # æ¨¡æ‹Ÿæˆäº¤é‡ä»…ç”¨äºæµ‹è¯•
            }, index=dates)
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
    
    return market_data

def test_memory_optimization(num_symbols: int, days: int, scenario_name: str) -> Dict:
    """æµ‹è¯•å†…å­˜ä¼˜åŒ–"""
    print(f"\n{'='*60}")
    print(f"å†…å­˜ä¼˜åŒ–æµ‹è¯•: {scenario_name}")
    print(f"è‚¡ç¥¨æ•°é‡: {num_symbols}, æ•°æ®å¤©æ•°: {days}")
    print(f"{'='*60}")
    
    monitor = MemoryMonitor()
    monitor.start_monitoring()
    
    try:
        # åˆå§‹å†…å­˜å¿«ç…§
        monitor.take_snapshot("å¼€å§‹")
        initial_memory = monitor.get_memory_usage()
        print(f"åˆå§‹å†…å­˜ä½¿ç”¨: {initial_memory['rss_mb']:.1f} MB")
        
        # ç”Ÿæˆæ•°æ®
        start_time = time.time()
        market_data = create_memory_efficient_data(num_symbols, days)
        data_time = time.time() - start_time
        
        monitor.take_snapshot("æ•°æ®ç”Ÿæˆå®Œæˆ")
        data_memory = monitor.get_memory_usage()
        print(f"æ•°æ®ç”Ÿæˆåå†…å­˜: {data_memory['rss_mb']:.1f} MB (+{data_memory['rss_mb'] - initial_memory['rss_mb']:.1f} MB)")
        
        # åˆ›å»ºå›æµ‹å¼•æ“
        parallel_engine = ParallelBacktestEngine()
        
        # æ‰¹é‡æ·»åŠ å¼•æ“ä»¥å‡å°‘å†…å­˜å³°å€¼
        batch_size = 5
        for batch_start in range(0, len(market_data), batch_size):
            batch_symbols = list(market_data.keys())[batch_start:batch_start + batch_size]
            
            for symbol in batch_symbols:
                data = market_data[symbol]
                engine = EnhancedBacktestEngine(
                    initial_capital=1000000,
                    commission_rate=0.001,
                    slippage_rate=0.001
                )
                engine.set_market_data(symbol, data)
                
                strategy = SimpleMovingAverageStrategy(f"SMA_{symbol}", short_window=10, long_window=30)
                engine.add_strategy(strategy)
                
                parallel_engine.add_engine(symbol, engine)
            
            # æ¯æ‰¹æ¬¡åè¿›è¡Œåƒåœ¾å›æ”¶
            gc.collect()
        
        monitor.take_snapshot("å¼•æ“åˆ›å»ºå®Œæˆ")
        engine_memory = monitor.get_memory_usage()
        print(f"å¼•æ“åˆ›å»ºåå†…å­˜: {engine_memory['rss_mb']:.1f} MB (+{engine_memory['rss_mb'] - data_memory['rss_mb']:.1f} MB)")
        
        # æ‰§è¡Œå›æµ‹
        print("å¼€å§‹å›æµ‹...")
        backtest_start = time.time()
        
        start_date = min(data.index[0] for data in market_data.values())
        end_date = max(data.index[-1] for data in market_data.values())
        
        results = parallel_engine.run_parallel_backtest(start_date, end_date)
        
        backtest_time = time.time() - backtest_start
        
        monitor.take_snapshot("å›æµ‹å®Œæˆ")
        final_memory = monitor.get_memory_usage()
        print(f"å›æµ‹å®Œæˆåå†…å­˜: {final_memory['rss_mb']:.1f} MB (+{final_memory['rss_mb'] - engine_memory['rss_mb']:.1f} MB)")
        
        # æ¸…ç†å†…å­˜
        del market_data
        del parallel_engine
        del results
        gc.collect()
        
        monitor.take_snapshot("æ¸…ç†å®Œæˆ")
        cleanup_memory = monitor.get_memory_usage()
        print(f"æ¸…ç†åå†…å­˜: {cleanup_memory['rss_mb']:.1f} MB ({cleanup_memory['rss_mb'] - initial_memory['rss_mb']:.1f} MB å‡€å¢é•¿)")
        
        # åˆ†æå†…å­˜ä½¿ç”¨
        memory_analysis = monitor.analyze_memory_growth()
        top_consumers = monitor.get_top_memory_consumers(5)
        
        print(f"\nå†…å­˜åˆ†æ:")
        print(f"- å³°å€¼å†…å­˜: {memory_analysis['peak_rss_mb']:.1f} MB")
        print(f"- æ€»å¢é•¿: {memory_analysis['growth_rss_mb']:.1f} MB ({memory_analysis['growth_percent']:.1f}%)")
        print(f"- æ•°æ®ç”Ÿæˆè€—æ—¶: {data_time:.2f} ç§’")
        print(f"- å›æµ‹æ‰§è¡Œè€—æ—¶: {backtest_time:.2f} ç§’")
        
        return {
            'scenario': scenario_name,
            'num_symbols': num_symbols,
            'days': days,
            'initial_memory_mb': initial_memory['rss_mb'],
            'peak_memory_mb': memory_analysis['peak_rss_mb'],
            'final_memory_mb': cleanup_memory['rss_mb'],
            'memory_growth_mb': memory_analysis['growth_rss_mb'],
            'memory_growth_percent': memory_analysis['growth_percent'],
            'data_time': data_time,
            'backtest_time': backtest_time,
            'total_time': data_time + backtest_time,
            'top_consumers': top_consumers,
            'memory_snapshots': memory_analysis['snapshots']
        }
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        return {
            'scenario': scenario_name,
            'error': str(e)
        }

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§  å†…å­˜ä¼˜åŒ–æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•åœºæ™¯
    test_scenarios = [
        (10, 252, "åŸºå‡†æµ‹è¯• (10è‚¡ç¥¨, 1å¹´)"),
        (25, 252, "ä¸­ç­‰è§„æ¨¡ (25è‚¡ç¥¨, 1å¹´)"),
        (50, 252, "å¤§è§„æ¨¡ (50è‚¡ç¥¨, 1å¹´)"),
        (100, 252, "è¶…å¤§è§„æ¨¡ (100è‚¡ç¥¨, 1å¹´)"),
    ]
    
    results = []
    
    for num_symbols, days, scenario_name in test_scenarios:
        result = test_memory_optimization(num_symbols, days, scenario_name)
        results.append(result)
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶å’ŒçŸ­æš‚ä¼‘æ¯
        gc.collect()
        time.sleep(2)
    
    # è¾“å‡ºæ±‡æ€»ç»“æœ
    print(f"\n{'='*80}")
    print("å†…å­˜ä¼˜åŒ–æµ‹è¯•ç»“æœæ±‡æ€»")
    print(f"{'='*80}")
    
    for result in results:
        if 'error' in result:
            print(f"âŒ {result['scenario']}: å¤±è´¥ - {result['error']}")
        else:
            print(f"âœ… {result['scenario']}:")
            print(f"   - å³°å€¼å†…å­˜: {result['peak_memory_mb']:.1f} MB")
            print(f"   - å†…å­˜å¢é•¿: {result['memory_growth_mb']:.1f} MB ({result['memory_growth_percent']:.1f}%)")
            print(f"   - æ€»è€—æ—¶: {result['total_time']:.2f}s")
            print(f"   - å†…å­˜æ•ˆç‡: {result['num_symbols']/result['peak_memory_mb']:.2f} è‚¡ç¥¨/MB")
    
    # å†…å­˜ä¼˜åŒ–å»ºè®®
    print(f"\nğŸ’¡ å†…å­˜ä¼˜åŒ–å»ºè®®:")
    print("1. ä½¿ç”¨ float32 è€Œä¸æ˜¯ float64 å¯èŠ‚çœçº¦50%å†…å­˜")
    print("2. åˆ†æ‰¹å¤„ç†å¤§æ•°æ®é›†ï¼Œé¿å…å†…å­˜å³°å€¼")
    print("3. åŠæ—¶è°ƒç”¨ gc.collect() é‡Šæ”¾ä¸éœ€è¦çš„å¯¹è±¡")
    print("4. ä½¿ç”¨é€‚å½“çš„æ•°æ®ç±»å‹ (int32 vs int64)")
    print("5. è€ƒè™‘ä½¿ç”¨æ•°æ®æµå¤„ç†è€Œä¸æ˜¯ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®")
    
    print(f"\nğŸ‰ å†…å­˜ä¼˜åŒ–æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main()