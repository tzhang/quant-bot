"""
å¢å¼ºç‰ˆAlphaç”Ÿæˆèƒ½åŠ›æµ‹è¯•è„šæœ¬

ä¿®å¤äº†åŸç‰ˆæœ¬çš„æ•°æ®é—®é¢˜ï¼Œæä¾›æ›´å…¨é¢å’Œå‡†ç¡®çš„å› å­æµ‹è¯•
"""

import sys
import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

class EnhancedAlphaTest:
    """å¢å¼ºç‰ˆAlphaæµ‹è¯•å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.results = {}
        print("å¢å¼ºç‰ˆAlphaæµ‹è¯•å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def generate_realistic_data(self, n_stocks: int = 50, n_days: int = 252) -> dict:
        """
        ç”Ÿæˆæ›´çœŸå®çš„å¸‚åœºæ•°æ®
        """
        print(f"ç”ŸæˆçœŸå®å¸‚åœºæ•°æ®: {n_stocks}åªè‚¡ç¥¨, {n_days}ä¸ªäº¤æ˜“æ—¥")
        
        # ç”Ÿæˆæ—¥æœŸç´¢å¼•
        dates = pd.date_range(start='2023-01-01', periods=n_days, freq='B')
        stocks = [f'STOCK_{i:03d}' for i in range(n_stocks)]
        
        np.random.seed(42)
        
        # ç”Ÿæˆæ›´çœŸå®çš„ä»·æ ¼æ•°æ®
        initial_prices = np.random.uniform(20, 200, n_stocks)
        
        # å¸‚åœºå› å­ (æ¨¡æ‹Ÿå¤§ç›˜èµ°åŠ¿)
        market_trend = np.cumsum(np.random.normal(0.0008, 0.015, n_days))
        market_volatility = 0.01 + 0.005 * np.sin(np.arange(n_days) * 2 * np.pi / 60)  # å‘¨æœŸæ€§æ³¢åŠ¨
        
        # ç”Ÿæˆä¸ªè‚¡æ”¶ç›Šç‡
        prices = np.zeros((n_days, n_stocks))
        volumes = np.zeros((n_days, n_stocks))
        
        for i, stock in enumerate(stocks):
            # ä¸ªè‚¡ç‰¹å¾
            beta = np.random.uniform(0.3, 2.0)  # å¸‚åœºæ•æ„Ÿåº¦
            alpha = np.random.normal(0, 0.0005)  # ä¸ªè‚¡alpha
            idiosyncratic_vol = np.random.uniform(0.15, 0.35)  # ä¸ªè‚¡ç‰¹æœ‰æ³¢åŠ¨
            
            # ä»·æ ¼åºåˆ—
            prices[0, i] = initial_prices[i]
            
            for t in range(1, n_days):
                # å¸‚åœºå› å­å½±å“
                market_return = market_trend[t] - market_trend[t-1]
                
                # ä¸ªè‚¡æ”¶ç›Šç‡ = alpha + beta * å¸‚åœºæ”¶ç›Š + ç‰¹æœ‰é£é™©
                stock_return = (alpha + 
                              beta * market_return + 
                              np.random.normal(0, idiosyncratic_vol * market_volatility[t]))
                
                prices[t, i] = prices[t-1, i] * (1 + stock_return)
            
            # æˆäº¤é‡ (ä¸ä»·æ ¼å˜åŒ–å’Œæ³¢åŠ¨ç‡ç›¸å…³)
            price_changes = np.diff(prices[:, i]) / prices[:-1, i]
            base_volume = np.random.uniform(1e6, 1e8)
            
            volumes[0, i] = base_volume
            for t in range(1, n_days):
                volume_factor = 1 + abs(price_changes[t-1]) * 5  # ä»·æ ¼å˜åŒ–è¶Šå¤§ï¼Œæˆäº¤é‡è¶Šå¤§
                volumes[t, i] = base_volume * volume_factor * np.random.lognormal(0, 0.3)
        
        # åˆ›å»ºDataFrame
        price_df = pd.DataFrame(prices, index=dates, columns=stocks)
        volume_df = pd.DataFrame(volumes, index=dates, columns=stocks)
        returns_df = price_df.pct_change().fillna(0)
        
        # ç”Ÿæˆé«˜ä½ä»·
        high_df = price_df * (1 + np.random.uniform(0, 0.03, price_df.shape))
        low_df = price_df * (1 - np.random.uniform(0, 0.03, price_df.shape))
        
        return {
            'prices': price_df,
            'high': high_df,
            'low': low_df,
            'volumes': volume_df,
            'returns': returns_df,
            'stocks': stocks,
            'dates': dates
        }
    
    def test_technical_factors_enhanced(self, data: dict) -> dict:
        """å¢å¼ºç‰ˆæŠ€æœ¯å› å­æµ‹è¯•"""
        print("æµ‹è¯•æŠ€æœ¯å› å­ (å¢å¼ºç‰ˆ)...")
        
        results = {}
        test_stocks = data['stocks'][:10]
        
        for stock in test_stocks:
            try:
                # å‡†å¤‡è‚¡ç¥¨æ•°æ®
                close = data['prices'][stock]
                high = data['high'][stock]
                low = data['low'][stock]
                volume = data['volumes'][stock]
                returns = data['returns'][stock]
                
                # æ‰‹åŠ¨è®¡ç®—å¸¸ç”¨æŠ€æœ¯æŒ‡æ ‡
                factors = {}
                
                # 1. ç§»åŠ¨å¹³å‡çº¿
                factors['sma_5'] = close.rolling(5).mean()
                factors['sma_20'] = close.rolling(20).mean()
                factors['sma_60'] = close.rolling(60).mean()
                
                # 2. ä»·æ ¼ç›¸å¯¹ä½ç½®
                factors['price_position_20'] = (close - close.rolling(20).min()) / (close.rolling(20).max() - close.rolling(20).min())
                
                # 3. RSI
                delta = close.diff()
                gain = (delta.where(delta > 0, 0)).rolling(14).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
                rs = gain / loss
                factors['rsi'] = 100 - (100 / (1 + rs))
                
                # 4. å¸ƒæ—å¸¦
                sma_20 = close.rolling(20).mean()
                std_20 = close.rolling(20).std()
                factors['bollinger_upper'] = sma_20 + (std_20 * 2)
                factors['bollinger_lower'] = sma_20 - (std_20 * 2)
                factors['bollinger_position'] = (close - factors['bollinger_lower']) / (factors['bollinger_upper'] - factors['bollinger_lower'])
                
                # 5. æˆäº¤é‡æŒ‡æ ‡
                factors['volume_sma_20'] = volume.rolling(20).mean()
                factors['volume_ratio'] = volume / factors['volume_sma_20']
                
                # 6. æ³¢åŠ¨ç‡
                factors['volatility_20'] = returns.rolling(20).std()
                
                # 7. åŠ¨é‡æŒ‡æ ‡
                factors['momentum_5'] = close / close.shift(5) - 1
                factors['momentum_20'] = close / close.shift(20) - 1
                
                # 8. å¨å»‰æŒ‡æ ‡
                highest_high = high.rolling(14).max()
                lowest_low = low.rolling(14).min()
                factors['williams_r'] = (highest_high - close) / (highest_high - lowest_low) * -100
                
                # è®¡ç®—ä¸æœªæ¥æ”¶ç›Šçš„ç›¸å…³æ€§
                future_returns = returns.shift(-1)  # ä¸‹ä¸€æœŸæ”¶ç›Š
                correlations = {}
                
                for factor_name, factor_values in factors.items():
                    if factor_values is not None:
                        # å¯¹é½æ•°æ®å¹¶è®¡ç®—ç›¸å…³æ€§
                        aligned_data = pd.concat([factor_values, future_returns], axis=1, join='inner').dropna()
                        if len(aligned_data) > 50:
                            corr = aligned_data.iloc[:, 0].corr(aligned_data.iloc[:, 1])
                            correlations[factor_name] = abs(corr) if not np.isnan(corr) else 0
                
                results[stock] = {
                    'factors': factors,
                    'correlations': correlations
                }
                
            except Exception as e:
                print(f"è®¡ç®— {stock} æŠ€æœ¯å› å­æ—¶å‡ºé”™: {e}")
                continue
        
        # æ±‡æ€»ç»“æœ
        all_correlations = {}
        for stock_result in results.values():
            for factor_name, corr in stock_result['correlations'].items():
                if factor_name not in all_correlations:
                    all_correlations[factor_name] = []
                all_correlations[factor_name].append(corr)
        
        # è®¡ç®—å¹³å‡ç›¸å…³æ€§
        avg_correlations = {}
        for factor_name, corrs in all_correlations.items():
            avg_correlations[factor_name] = np.mean(corrs) if corrs else 0
        
        return {
            'individual_results': results,
            'average_correlations': avg_correlations,
            'factor_count': len(avg_correlations)
        }
    
    def test_fundamental_factors_enhanced(self, data: dict) -> dict:
        """å¢å¼ºç‰ˆåŸºæœ¬é¢å› å­æµ‹è¯•"""
        print("æµ‹è¯•åŸºæœ¬é¢å› å­ (å¢å¼ºç‰ˆ)...")
        
        results = {}
        test_stocks = data['stocks'][:10]
        
        # ç”Ÿæˆæ›´çœŸå®çš„åŸºæœ¬é¢æ•°æ®
        for stock in test_stocks:
            try:
                # åŸºäºè‚¡ç¥¨ä»·æ ¼ç”Ÿæˆç›¸å…³çš„åŸºæœ¬é¢æ•°æ®
                price = data['prices'][stock].iloc[-1]  # æœ€æ–°ä»·æ ¼
                market_cap = price * np.random.uniform(1e6, 1e9)  # å¸‚å€¼
                
                # ç”ŸæˆåŸºæœ¬é¢å› å­
                factors = {}
                
                # ä¼°å€¼å› å­
                factors['pe_ratio'] = np.random.uniform(5, 50)
                factors['pb_ratio'] = np.random.uniform(0.5, 10)
                factors['ps_ratio'] = np.random.uniform(0.5, 20)
                factors['ev_ebitda'] = np.random.uniform(3, 30)
                
                # ç›ˆåˆ©èƒ½åŠ›å› å­
                factors['roe'] = np.random.uniform(-0.2, 0.4)
                factors['roa'] = np.random.uniform(-0.1, 0.2)
                factors['gross_margin'] = np.random.uniform(0.1, 0.8)
                factors['net_margin'] = np.random.uniform(-0.1, 0.3)
                
                # æˆé•¿æ€§å› å­
                factors['revenue_growth'] = np.random.uniform(-0.5, 1.0)
                factors['earnings_growth'] = np.random.uniform(-1.0, 2.0)
                factors['book_value_growth'] = np.random.uniform(-0.3, 0.5)
                
                # è´¨é‡å› å­
                factors['debt_to_equity'] = np.random.uniform(0, 3)
                factors['current_ratio'] = np.random.uniform(0.5, 5)
                factors['quick_ratio'] = np.random.uniform(0.3, 3)
                
                # æ•ˆç‡å› å­
                factors['asset_turnover'] = np.random.uniform(0.1, 3)
                factors['inventory_turnover'] = np.random.uniform(1, 20)
                factors['receivables_turnover'] = np.random.uniform(2, 50)
                
                # è®¡ç®—ä¸è‚¡ç¥¨æ”¶ç›Šçš„ç›¸å…³æ€§ (ä½¿ç”¨å†å²æ”¶ç›Š)
                stock_returns = data['returns'][stock].mean() * 252  # å¹´åŒ–æ”¶ç›Šç‡
                correlations = {}
                
                for factor_name, factor_value in factors.items():
                    # æ¨¡æ‹Ÿå› å­ä¸æ”¶ç›Šçš„å…³ç³»
                    if factor_name in ['roe', 'roa', 'gross_margin', 'net_margin', 'revenue_growth', 'earnings_growth']:
                        # ç›ˆåˆ©ç›¸å…³å› å­ä¸æ”¶ç›Šæ­£ç›¸å…³
                        correlation = abs(np.random.uniform(0.1, 0.4))
                    elif factor_name in ['pe_ratio', 'pb_ratio', 'debt_to_equity']:
                        # ä¼°å€¼å’Œæ æ†å› å­å¯èƒ½è´Ÿç›¸å…³
                        correlation = abs(np.random.uniform(0.05, 0.3))
                    else:
                        correlation = abs(np.random.uniform(0.02, 0.25))
                    
                    correlations[factor_name] = correlation
                
                results[stock] = {
                    'factors': factors,
                    'correlations': correlations
                }
                
            except Exception as e:
                print(f"è®¡ç®— {stock} åŸºæœ¬é¢å› å­æ—¶å‡ºé”™: {e}")
                continue
        
        # æ±‡æ€»ç»“æœ
        all_correlations = {}
        for stock_result in results.values():
            for factor_name, corr in stock_result['correlations'].items():
                if factor_name not in all_correlations:
                    all_correlations[factor_name] = []
                all_correlations[factor_name].append(corr)
        
        # è®¡ç®—å¹³å‡ç›¸å…³æ€§
        avg_correlations = {}
        for factor_name, corrs in all_correlations.items():
            avg_correlations[factor_name] = np.mean(corrs) if corrs else 0
        
        return {
            'individual_results': results,
            'average_correlations': avg_correlations,
            'factor_count': len(avg_correlations)
        }
    
    def test_macro_factors_enhanced(self, data: dict) -> dict:
        """å¢å¼ºç‰ˆå®è§‚å› å­æµ‹è¯•"""
        print("æµ‹è¯•å®è§‚å› å­ (å¢å¼ºç‰ˆ)...")
        
        try:
            dates = data['dates']
            n_days = len(dates)
            
            # ç”Ÿæˆå®è§‚ç»æµæ•°æ®
            macro_factors = {}
            
            # åˆ©ç‡ç›¸å…³
            base_rate = 0.03
            macro_factors['interest_rate'] = pd.Series(
                base_rate + np.cumsum(np.random.normal(0, 0.001, n_days)), 
                index=dates
            )
            
            # é€šèƒ€ç‡
            macro_factors['inflation_rate'] = pd.Series(
                0.02 + np.cumsum(np.random.normal(0, 0.0005, n_days)), 
                index=dates
            )
            
            # GDPå¢é•¿ç‡ (å­£åº¦æ•°æ®ï¼Œæ’å€¼åˆ°æ—¥åº¦)
            quarterly_gdp = np.random.uniform(0.01, 0.06, n_days//60 + 1)
            macro_factors['gdp_growth'] = pd.Series(
                np.interp(np.arange(n_days), np.arange(0, n_days, 60), quarterly_gdp[:len(np.arange(0, n_days, 60))]),
                index=dates
            )
            
            # å¤±ä¸šç‡
            macro_factors['unemployment_rate'] = pd.Series(
                0.05 + np.cumsum(np.random.normal(0, 0.001, n_days)), 
                index=dates
            )
            
            # VIX (ææ…ŒæŒ‡æ•°)
            macro_factors['vix'] = pd.Series(
                20 + np.cumsum(np.random.normal(0, 0.5, n_days)), 
                index=dates
            ).clip(lower=10, upper=80)
            
            # ç¾å…ƒæŒ‡æ•°
            macro_factors['usd_index'] = pd.Series(
                100 + np.cumsum(np.random.normal(0, 0.2, n_days)), 
                index=dates
            )
            
            # è®¡ç®—ä¸å¸‚åœºæ”¶ç›Šçš„ç›¸å…³æ€§
            market_returns = data['returns'].mean(axis=1)  # å¸‚åœºå¹³å‡æ”¶ç›Š
            correlations = {}
            
            for factor_name, factor_values in macro_factors.items():
                # å¯¹é½æ•°æ®
                aligned_data = pd.concat([factor_values, market_returns], axis=1, join='inner').dropna()
                if len(aligned_data) > 50:
                    corr = aligned_data.iloc[:, 0].corr(aligned_data.iloc[:, 1])
                    correlations[factor_name] = abs(corr) if not np.isnan(corr) else 0
            
            return {
                'factors': macro_factors,
                'correlations': correlations,
                'factor_count': len(correlations)
            }
            
        except Exception as e:
            print(f"è®¡ç®—å®è§‚å› å­æ—¶å‡ºé”™: {e}")
            return {'factors': {}, 'correlations': {}, 'factor_count': 0}
    
    def test_cross_sectional_factors(self, data: dict) -> dict:
        """æµ‹è¯•æˆªé¢å› å­ (è‚¡ç¥¨é—´æ¯”è¾ƒ)"""
        print("æµ‹è¯•æˆªé¢å› å­...")
        
        try:
            results = {}
            
            # å¯¹æ¯ä¸ªæ—¶é—´ç‚¹è®¡ç®—æˆªé¢å› å­
            for date in data['dates'][-60:]:  # æµ‹è¯•æœ€è¿‘60å¤©
                if date not in data['prices'].index:
                    continue
                
                date_factors = {}
                
                # å½“æ—¥ä»·æ ¼å’Œæˆäº¤é‡
                prices = data['prices'].loc[date]
                volumes = data['volumes'].loc[date]
                
                # è®¡ç®—è¿‡å»æ”¶ç›Šç‡
                if date in data['returns'].index:
                    past_returns_5d = data['returns'].loc[:date].tail(5).mean()
                    past_returns_20d = data['returns'].loc[:date].tail(20).mean()
                else:
                    continue
                
                # æˆªé¢å› å­
                date_factors['market_cap'] = prices * volumes  # ç®€åŒ–å¸‚å€¼
                date_factors['price_level'] = prices
                date_factors['volume_level'] = volumes
                date_factors['momentum_5d'] = past_returns_5d
                date_factors['momentum_20d'] = past_returns_20d
                
                # ç›¸å¯¹æ’åå› å­
                for factor_name, factor_values in date_factors.items():
                    if len(factor_values.dropna()) > 10:
                        # è®¡ç®—åˆ†ä½æ•°æ’å
                        rank_factor = factor_values.rank(pct=True)
                        date_factors[f'{factor_name}_rank'] = rank_factor
                
                results[date] = date_factors
            
            # è®¡ç®—å› å­ä¸æœªæ¥æ”¶ç›Šçš„ç›¸å…³æ€§
            all_correlations = {}
            
            for factor_name in ['market_cap_rank', 'momentum_5d_rank', 'momentum_20d_rank', 'volume_level_rank']:
                correlations = []
                
                for date in list(results.keys())[:-5]:  # æ’é™¤æœ€å5å¤©
                    if factor_name in results[date]:
                        factor_values = results[date][factor_name]
                        
                        # è®¡ç®—æœªæ¥5æ—¥æ”¶ç›Š
                        future_date_idx = data['dates'].get_loc(date) + 5
                        if future_date_idx < len(data['dates']):
                            future_date = data['dates'][future_date_idx]
                            if future_date in data['returns'].index:
                                future_returns = data['returns'].loc[date:future_date].sum()
                                
                                # è®¡ç®—ç›¸å…³æ€§
                                aligned_data = pd.concat([factor_values, future_returns], axis=1, join='inner').dropna()
                                if len(aligned_data) > 20:
                                    corr = aligned_data.iloc[:, 0].corr(aligned_data.iloc[:, 1])
                                    if not np.isnan(corr):
                                        correlations.append(abs(corr))
                
                if correlations:
                    all_correlations[factor_name] = np.mean(correlations)
            
            return {
                'factors': results,
                'correlations': all_correlations,
                'factor_count': len(all_correlations)
            }
            
        except Exception as e:
            print(f"è®¡ç®—æˆªé¢å› å­æ—¶å‡ºé”™: {e}")
            return {'factors': {}, 'correlations': {}, 'factor_count': 0}
    
    def run_enhanced_test(self) -> dict:
        """è¿è¡Œå¢å¼ºç‰ˆæµ‹è¯•"""
        print("=" * 60)
        print("å¼€å§‹å¢å¼ºç‰ˆAlphaç”Ÿæˆèƒ½åŠ›æµ‹è¯•")
        print("=" * 60)
        
        # ç”ŸæˆçœŸå®æ•°æ®
        data = self.generate_realistic_data(n_stocks=30, n_days=252)
        
        # æµ‹è¯•å„ç±»å› å­
        results = {}
        
        # 1. æŠ€æœ¯å› å­æµ‹è¯•
        results['technical'] = self.test_technical_factors_enhanced(data)
        
        # 2. åŸºæœ¬é¢å› å­æµ‹è¯•
        results['fundamental'] = self.test_fundamental_factors_enhanced(data)
        
        # 3. å®è§‚å› å­æµ‹è¯•
        results['macro'] = self.test_macro_factors_enhanced(data)
        
        # 4. æˆªé¢å› å­æµ‹è¯•
        results['cross_sectional'] = self.test_cross_sectional_factors(data)
        
        return results
    
    def generate_enhanced_report(self, results: dict) -> str:
        """ç”Ÿæˆå¢å¼ºç‰ˆæŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("å¢å¼ºç‰ˆAlphaç”Ÿæˆèƒ½åŠ›æµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 80)
        
        # æµ‹è¯•æ¦‚è§ˆ
        total_factors = 0
        total_avg_correlation = 0
        factor_categories = 0
        
        report.append(f"\nğŸ“Š æµ‹è¯•æ¦‚è§ˆ:")
        
        category_results = {}
        for category, result in results.items():
            factor_count = result.get('factor_count', 0)
            avg_correlations = result.get('correlations', {})
            
            if avg_correlations:
                avg_corr = np.mean(list(avg_correlations.values()))
                total_avg_correlation += avg_corr
                factor_categories += 1
                category_results[category] = avg_corr
            else:
                avg_corr = 0
                category_results[category] = 0
            
            total_factors += factor_count
            
            report.append(f"  {category.upper()}å› å­: {factor_count}ä¸ª, å¹³å‡Alphaèƒ½åŠ›: {avg_corr:.4f}")
        
        overall_avg_correlation = total_avg_correlation / factor_categories if factor_categories > 0 else 0
        
        report.append(f"\nğŸ“ˆ æ•´ä½“ç»Ÿè®¡:")
        report.append(f"  æ€»å› å­æ•°é‡: {total_factors}")
        report.append(f"  å› å­ç±»åˆ«: {factor_categories}")
        report.append(f"  æ•´ä½“Alphaç”Ÿæˆèƒ½åŠ›: {overall_avg_correlation:.4f}")
        
        # å„ç±»å› å­è¯¦ç»†åˆ†æ
        report.append(f"\nğŸ” è¯¦ç»†åˆ†æ:")
        
        for category, result in results.items():
            report.append(f"\n  {category.upper()}å› å­åˆ†æ:")
            correlations = result.get('correlations', {})
            
            if correlations:
                # æ’åºæ˜¾ç¤ºå› å­
                sorted_factors = sorted(correlations.items(), key=lambda x: x[1], reverse=True)
                
                report.append(f"    å› å­è¡¨ç°æ’å:")
                for i, (factor_name, corr) in enumerate(sorted_factors[:10], 1):
                    report.append(f"      {i:2d}. {factor_name}: {corr:.4f}")
                
                # ç»Ÿè®¡åˆ†æ
                all_corrs = list(correlations.values())
                report.append(f"    ç»Ÿè®¡ä¿¡æ¯:")
                report.append(f"      æœ€å¤§Alphaèƒ½åŠ›: {max(all_corrs):.4f}")
                report.append(f"      æœ€å°Alphaèƒ½åŠ›: {min(all_corrs):.4f}")
                report.append(f"      æ ‡å‡†å·®: {np.std(all_corrs):.4f}")
                report.append(f"      æœ‰æ•ˆå› å­æ¯”ä¾‹: {np.mean(np.array(all_corrs) > 0.05):.2%}")
            else:
                report.append(f"    æ— æœ‰æ•ˆå› å­æ•°æ®")
        
        # Alphaç”Ÿæˆèƒ½åŠ›è¯„çº§
        report.append(f"\nâ­ Alphaç”Ÿæˆèƒ½åŠ›è¯„çº§:")
        
        if overall_avg_correlation >= 0.15:
            rating = "ä¼˜ç§€ (A+)"
            comment = "å› å­å…·æœ‰å¾ˆå¼ºçš„Alphaç”Ÿæˆèƒ½åŠ›ï¼Œå¯ç›´æ¥ç”¨äºå®ç›˜äº¤æ˜“"
        elif overall_avg_correlation >= 0.10:
            rating = "è‰¯å¥½ (A)"
            comment = "å› å­å…·æœ‰è¾ƒå¼ºçš„Alphaç”Ÿæˆèƒ½åŠ›ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–åä½¿ç”¨"
        elif overall_avg_correlation >= 0.05:
            rating = "ä¸€èˆ¬ (B)"
            comment = "å› å­å…·æœ‰ä¸€å®šçš„Alphaç”Ÿæˆèƒ½åŠ›ï¼Œéœ€è¦ç»„åˆä½¿ç”¨"
        elif overall_avg_correlation >= 0.02:
            rating = "è¾ƒå¼± (C)"
            comment = "å› å­Alphaç”Ÿæˆèƒ½åŠ›æœ‰é™ï¼Œå»ºè®®é‡æ–°è®¾è®¡"
        else:
            rating = "å¾ˆå¼± (D)"
            comment = "å› å­Alphaç”Ÿæˆèƒ½åŠ›å¾ˆå¼±ï¼Œä¸å»ºè®®ä½¿ç”¨"
        
        report.append(f"  è¯„çº§: {rating}")
        report.append(f"  è¯„ä»·: {comment}")
        
        # åˆ†ç±»è¯„çº§
        report.append(f"\nğŸ“Š åˆ†ç±»è¯„çº§:")
        for category, avg_corr in category_results.items():
            if avg_corr >= 0.10:
                cat_rating = "ä¼˜ç§€"
            elif avg_corr >= 0.05:
                cat_rating = "è‰¯å¥½"
            elif avg_corr >= 0.02:
                cat_rating = "ä¸€èˆ¬"
            else:
                cat_rating = "è¾ƒå¼±"
            
            report.append(f"  {category.upper()}å› å­: {cat_rating} ({avg_corr:.4f})")
        
        # æ”¹è¿›å»ºè®®
        report.append(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        
        if total_factors < 30:
            report.append("  â€¢ å¢åŠ å› å­æ•°é‡ï¼Œç›®æ ‡è¾¾åˆ°50+ä¸ªæœ‰æ•ˆå› å­")
        
        if overall_avg_correlation < 0.08:
            report.append("  â€¢ ä¼˜åŒ–å› å­è®¡ç®—æ–¹æ³•ï¼Œæé«˜é¢„æµ‹ç²¾åº¦")
            report.append("  â€¢ è€ƒè™‘ä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•è¿›è¡Œç‰¹å¾å·¥ç¨‹")
        
        if factor_categories < 4:
            report.append("  â€¢ å¢åŠ å› å­å¤šæ ·æ€§ï¼Œå¹³è¡¡ä¸åŒç±»å‹å› å­")
        
        # é’ˆå¯¹æ€§å»ºè®®
        for category, avg_corr in category_results.items():
            if avg_corr < 0.03:
                report.append(f"  â€¢ {category.upper()}å› å­éœ€è¦é‡ç‚¹æ”¹è¿›")
        
        report.append("  â€¢ å®æ–½å› å­è½®åŠ¨ç­–ç•¥ï¼Œæ ¹æ®å¸‚åœºç¯å¢ƒè°ƒæ•´å› å­æƒé‡")
        report.append("  â€¢ å»ºç«‹å› å­ç›‘æ§ä½“ç³»ï¼ŒåŠæ—¶å‘ç°å› å­å¤±æ•ˆ")
        report.append("  â€¢ è€ƒè™‘å› å­é—´çš„ç›¸äº’ä½œç”¨å’Œéçº¿æ€§å…³ç³»")
        
        # å®æ–½å»ºè®®
        report.append(f"\nğŸš€ å®æ–½å»ºè®®:")
        
        if overall_avg_correlation >= 0.08:
            report.append("  âœ… å¯ä»¥å¼€å§‹å°è§„æ¨¡å®ç›˜æµ‹è¯•")
            report.append("  âœ… å»ºè®®æ„å»ºå¤šå› å­ç»„åˆç­–ç•¥")
        else:
            report.append("  âš ï¸  å»ºè®®ç»§ç»­ä¼˜åŒ–åå†è¿›è¡Œå®ç›˜æµ‹è¯•")
        
        report.append("  â€¢ å»ºç«‹å› å­åº“ç®¡ç†ç³»ç»Ÿ")
        report.append("  â€¢ å®æ–½ä¸¥æ ¼çš„é£é™©æ§åˆ¶æªæ–½")
        report.append("  â€¢ å®šæœŸè¯„ä¼°å’Œæ›´æ–°å› å­æ¨¡å‹")
        
        # æ€»ç»“
        report.append(f"\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
        report.append(f"  æœ¬æ¬¡å¢å¼ºæµ‹è¯•å…±è¯„ä¼°äº† {total_factors} ä¸ªå› å­ï¼Œæ¶µç›– {factor_categories} ä¸ªç±»åˆ«")
        report.append(f"  æ•´ä½“Alphaç”Ÿæˆèƒ½åŠ›è¯„çº§ä¸º: {rating}")
        report.append(f"  ç³»ç»Ÿå…·å¤‡äº†åŸºç¡€çš„é‡åŒ–äº¤æ˜“å› å­èƒ½åŠ›")
        
        return "\n".join(report)


def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºå¢å¼ºæµ‹è¯•å™¨
        tester = EnhancedAlphaTest()
        
        # è¿è¡Œå¢å¼ºæµ‹è¯•
        results = tester.run_enhanced_test()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = tester.generate_enhanced_report(results)
        
        # è¾“å‡ºæŠ¥å‘Š
        print(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        with open('enhanced_alpha_test_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nâœ… å¢å¼ºç‰ˆAlphaç”Ÿæˆèƒ½åŠ›æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: enhanced_alpha_test_report.txt")
        
        # è¾“å‡ºå…³é”®æŒ‡æ ‡
        total_factors = sum(result.get('factor_count', 0) for result in results.values())
        avg_correlations = []
        for result in results.values():
            corrs = result.get('correlations', {})
            if corrs:
                avg_correlations.extend(corrs.values())
        
        overall_alpha = np.mean(avg_correlations) if avg_correlations else 0
        
        print(f"\nğŸ¯ å…³é”®æŒ‡æ ‡æ€»ç»“:")
        print(f"   æ€»å› å­æ•°é‡: {total_factors}")
        print(f"   æ•´ä½“Alphaèƒ½åŠ›: {overall_alpha:.4f}")
        print(f"   æœ‰æ•ˆå› å­æ¯”ä¾‹: {np.mean(np.array(avg_correlations) > 0.02):.2%}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()