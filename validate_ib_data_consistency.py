#!/usr/bin/env python3
"""
IBæ•°æ®ä¸€è‡´æ€§éªŒè¯è„šæœ¬

è¯¥è„šæœ¬ç”¨äºéªŒè¯Interactive Brokersæ•°æ®ä¸å…¶ä»–æ•°æ®æºï¼ˆyfinanceã€qlibç­‰ï¼‰çš„ä¸€è‡´æ€§ã€‚
æ¯”è¾ƒä»·æ ¼æ•°æ®ã€æˆäº¤é‡ç­‰å…³é”®æŒ‡æ ‡çš„å·®å¼‚ã€‚

ä½œè€…: AI Assistant
æ—¥æœŸ: 2024
"""

import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.data.ib_data_provider import IBDataProvider, create_ib_provider
from src.data.qlib_data_provider import QlibDataProvider
import yfinance as yf

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DataConsistencyValidator:
    """æ•°æ®ä¸€è‡´æ€§éªŒè¯å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®æä¾›è€…"""
        self.ib_provider = create_ib_provider()
        self.qlib_provider = QlibDataProvider()
        
        # æµ‹è¯•è‚¡ç¥¨åˆ—è¡¨
        self.test_symbols = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA']
        
        # æ•°æ®ä¸€è‡´æ€§é˜ˆå€¼
        self.price_tolerance = 0.05  # 5% ä»·æ ¼å·®å¼‚å®¹å¿åº¦
        self.volume_tolerance = 0.10  # 10% æˆäº¤é‡å·®å¼‚å®¹å¿åº¦
        
    def get_data_from_all_sources(self, symbol: str, start_date: str, end_date: str) -> Dict[str, pd.DataFrame]:
        """ä»æ‰€æœ‰æ•°æ®æºè·å–æ•°æ®"""
        data_sources = {}
        
        # IBæ•°æ®
        try:
            if self.ib_provider.is_available:
                ib_data = self.ib_provider.get_stock_data(symbol, start_date, end_date)
                if not ib_data.empty:
                    data_sources['IB'] = ib_data
                    logger.info(f"âœ… IBæ•°æ®è·å–æˆåŠŸ: {symbol}, {len(ib_data)} æ¡è®°å½•")
                else:
                    logger.warning(f"âš ï¸ IBæ•°æ®ä¸ºç©º: {symbol}")
            else:
                logger.warning("âš ï¸ IBæ•°æ®æºä¸å¯ç”¨")
        except Exception as e:
            logger.error(f"âŒ IBæ•°æ®è·å–å¤±è´¥: {symbol}, é”™è¯¯: {e}")
        
        # yfinanceæ•°æ®
        try:
            ticker = yf.Ticker(symbol)
            # ä½¿ç”¨periodå‚æ•°æ›¿ä»£start/endæ—¥æœŸä»¥é¿å…æ—¥æœŸèŒƒå›´é—®é¢˜
            yf_data = ticker.history(period='1mo')
            
            if not yf_data.empty:
                # æ ‡å‡†åŒ–åˆ—åä¸ºå°å†™
                yf_data.columns = [col.lower() for col in yf_data.columns]
                data_sources['yfinance'] = yf_data
                logger.info(f"âœ… yfinanceæ•°æ®è·å–æˆåŠŸ: {symbol}, {len(yf_data)} æ¡è®°å½•")
            else:
                logger.warning(f"âš ï¸ yfinanceæ•°æ®ä¸ºç©º: {symbol}")
        except Exception as e:
            logger.error(f"âŒ yfinanceæ•°æ®è·å–å¤±è´¥: {symbol}, é”™è¯¯: {e}")
        
        # qlibæ•°æ®
        try:
            qlib_data = self.qlib_provider.get_stock_data(symbol, start_date, end_date)
            if not qlib_data.empty:
                data_sources['qlib'] = qlib_data
                logger.info(f"âœ… qlibæ•°æ®è·å–æˆåŠŸ: {symbol}, {len(qlib_data)} æ¡è®°å½•")
            else:
                logger.warning(f"âš ï¸ qlibæ•°æ®ä¸ºç©º: {symbol}")
        except Exception as e:
            logger.error(f"âŒ qlibæ•°æ®è·å–å¤±è´¥: {symbol}, é”™è¯¯: {e}")
        
        return data_sources
    
    def compare_price_data(self, data1: pd.DataFrame, data2: pd.DataFrame, 
                          source1: str, source2: str) -> Dict[str, float]:
        """æ¯”è¾ƒä¸¤ä¸ªæ•°æ®æºçš„ä»·æ ¼æ•°æ®"""
        # æ ‡å‡†åŒ–ç´¢å¼•ä¸ºæ—¥æœŸï¼ˆç§»é™¤æ—¶åŒºä¿¡æ¯ï¼‰
        data1_normalized = data1.copy()
        data2_normalized = data2.copy()
        
        # æ ‡å‡†åŒ–ç´¢å¼•
        if hasattr(data1_normalized.index, 'tz_localize'):
            data1_normalized.index = data1_normalized.index.tz_localize(None)
        if hasattr(data2_normalized.index, 'tz_localize'):
            data2_normalized.index = data2_normalized.index.tz_localize(None)
        
        # è½¬æ¢ä¸ºæ—¥æœŸç´¢å¼•
        data1_normalized.index = pd.to_datetime(data1_normalized.index.date)
        data2_normalized.index = pd.to_datetime(data2_normalized.index.date)
        
        # æ‰¾åˆ°å…±åŒçš„æ—¥æœŸ
        common_dates = data1_normalized.index.intersection(data2_normalized.index)
        
        if len(common_dates) == 0:
            return {
                'common_dates': 0,
                'close_correlation': 0.0,
                'close_mean_diff_pct': 100.0,
                'volume_correlation': 0.0,
                'volume_mean_diff_pct': 100.0
            }
        
        # è·å–å…±åŒæ—¥æœŸçš„æ•°æ®
        df1_common = data1_normalized.loc[common_dates]
        df2_common = data2_normalized.loc[common_dates]
        
        # ä»·æ ¼æ¯”è¾ƒ
        close_corr = df1_common['close'].corr(df2_common['close'])
        close_diff_pct = abs((df1_common['close'] - df2_common['close']) / df1_common['close'] * 100).mean()
        
        # æˆäº¤é‡æ¯”è¾ƒ
        volume_corr = 0.0
        volume_diff_pct = 100.0
        
        if 'volume' in df1_common.columns and 'volume' in df2_common.columns:
            # è¿‡æ»¤æ‰é›¶æˆäº¤é‡çš„æ•°æ®
            valid_volume = (df1_common['volume'] > 0) & (df2_common['volume'] > 0)
            if valid_volume.sum() > 0:
                volume_corr = df1_common.loc[valid_volume, 'volume'].corr(
                    df2_common.loc[valid_volume, 'volume']
                )
                volume_diff_pct = abs(
                    (df1_common.loc[valid_volume, 'volume'] - df2_common.loc[valid_volume, 'volume']) / 
                    df1_common.loc[valid_volume, 'volume'] * 100
                ).mean()
        
        return {
            'common_dates': len(common_dates),
            'close_correlation': close_corr if not np.isnan(close_corr) else 0.0,
            'close_mean_diff_pct': close_diff_pct if not np.isnan(close_diff_pct) else 100.0,
            'volume_correlation': volume_corr if not np.isnan(volume_corr) else 0.0,
            'volume_mean_diff_pct': volume_diff_pct if not np.isnan(volume_diff_pct) else 100.0
        }
    
    def validate_symbol_consistency(self, symbol: str, start_date: str, end_date: str) -> Dict:
        """éªŒè¯å•ä¸ªè‚¡ç¥¨çš„æ•°æ®ä¸€è‡´æ€§"""
        logger.info(f"å¼€å§‹éªŒè¯ {symbol} çš„æ•°æ®ä¸€è‡´æ€§...")
        
        # è·å–æ‰€æœ‰æ•°æ®æºçš„æ•°æ®
        data_sources = self.get_data_from_all_sources(symbol, start_date, end_date)
        
        if len(data_sources) < 2:
            logger.warning(f"âš ï¸ {symbol} å¯ç”¨æ•°æ®æºä¸è¶³2ä¸ªï¼Œæ— æ³•è¿›è¡Œä¸€è‡´æ€§éªŒè¯")
            return {
                'symbol': symbol,
                'available_sources': list(data_sources.keys()),
                'comparisons': {},
                'overall_consistency': 'insufficient_data'
            }
        
        # è¿›è¡Œä¸¤ä¸¤æ¯”è¾ƒ
        comparisons = {}
        source_names = list(data_sources.keys())
        
        for i in range(len(source_names)):
            for j in range(i + 1, len(source_names)):
                source1, source2 = source_names[i], source_names[j]
                comparison_key = f"{source1}_vs_{source2}"
                
                comparison_result = self.compare_price_data(
                    data_sources[source1], 
                    data_sources[source2],
                    source1, 
                    source2
                )
                
                comparisons[comparison_key] = comparison_result
                
                logger.info(f"ğŸ“Š {comparison_key}: "
                          f"å…±åŒæ—¥æœŸ={comparison_result['common_dates']}, "
                          f"ä»·æ ¼ç›¸å…³æ€§={comparison_result['close_correlation']:.3f}, "
                          f"ä»·æ ¼å·®å¼‚={comparison_result['close_mean_diff_pct']:.2f}%")
        
        # è¯„ä¼°æ•´ä½“ä¸€è‡´æ€§
        overall_consistency = self._evaluate_overall_consistency(comparisons)
        
        return {
            'symbol': symbol,
            'available_sources': source_names,
            'comparisons': comparisons,
            'overall_consistency': overall_consistency
        }
    
    def _evaluate_overall_consistency(self, comparisons: Dict) -> str:
        """è¯„ä¼°æ•´ä½“ä¸€è‡´æ€§"""
        if not comparisons:
            return 'no_comparisons'
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_correlation = np.mean([comp['close_correlation'] for comp in comparisons.values()])
        avg_price_diff = np.mean([comp['close_mean_diff_pct'] for comp in comparisons.values()])
        
        # ä¸€è‡´æ€§è¯„çº§
        if avg_correlation > 0.95 and avg_price_diff < self.price_tolerance * 100:
            return 'excellent'
        elif avg_correlation > 0.90 and avg_price_diff < self.price_tolerance * 200:
            return 'good'
        elif avg_correlation > 0.80 and avg_price_diff < self.price_tolerance * 400:
            return 'fair'
        else:
            return 'poor'
    
    def run_full_validation(self, start_date: str = None, end_date: str = None) -> Dict:
        """è¿è¡Œå®Œæ•´çš„æ•°æ®ä¸€è‡´æ€§éªŒè¯"""
        if start_date is None:
            start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
        if end_date is None:
            end_date = datetime.now().strftime('%Y-%m-%d')
        
        logger.info("=" * 80)
        logger.info("ğŸ” å¼€å§‹IBæ•°æ®ä¸€è‡´æ€§éªŒè¯")
        logger.info("=" * 80)
        logger.info(f"ğŸ“… éªŒè¯æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
        logger.info(f"ğŸ“ˆ æµ‹è¯•è‚¡ç¥¨: {', '.join(self.test_symbols)}")
        
        validation_results = {}
        consistency_summary = {
            'excellent': 0,
            'good': 0,
            'fair': 0,
            'poor': 0,
            'insufficient_data': 0
        }
        
        for symbol in self.test_symbols:
            try:
                result = self.validate_symbol_consistency(symbol, start_date, end_date)
                validation_results[symbol] = result
                consistency_summary[result['overall_consistency']] += 1
                
            except Exception as e:
                logger.error(f"âŒ {symbol} éªŒè¯å¤±è´¥: {e}")
                validation_results[symbol] = {
                    'symbol': symbol,
                    'error': str(e),
                    'overall_consistency': 'error'
                }
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_consistency_report(validation_results, consistency_summary)
        
        return {
            'validation_results': validation_results,
            'consistency_summary': consistency_summary,
            'test_period': f"{start_date} to {end_date}"
        }
    
    def _generate_consistency_report(self, results: Dict, summary: Dict):
        """ç”Ÿæˆä¸€è‡´æ€§éªŒè¯æŠ¥å‘Š"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š IBæ•°æ®ä¸€è‡´æ€§éªŒè¯æŠ¥å‘Š")
        logger.info("=" * 80)
        
        # æ€»ä½“ç»Ÿè®¡
        total_stocks = len(results)
        logger.info(f"ğŸ“ˆ æµ‹è¯•è‚¡ç¥¨æ€»æ•°: {total_stocks}")
        
        for level, count in summary.items():
            if count > 0:
                percentage = count / total_stocks * 100
                logger.info(f"   {level.upper()}: {count} åªè‚¡ç¥¨ ({percentage:.1f}%)")
        
        # è¯¦ç»†ç»“æœ
        logger.info("\nğŸ“‹ è¯¦ç»†éªŒè¯ç»“æœ:")
        for symbol, result in results.items():
            if 'error' in result:
                logger.info(f"   âŒ {symbol}: éªŒè¯å¤±è´¥ - {result['error']}")
                continue
                
            consistency = result['overall_consistency']
            sources = ', '.join(result['available_sources'])
            
            if consistency == 'excellent':
                emoji = "ğŸŸ¢"
            elif consistency == 'good':
                emoji = "ğŸŸ¡"
            elif consistency == 'fair':
                emoji = "ğŸŸ "
            else:
                emoji = "ğŸ”´"
            
            logger.info(f"   {emoji} {symbol}: {consistency.upper()} (æ•°æ®æº: {sources})")
            
            # æ˜¾ç¤ºæ¯”è¾ƒè¯¦æƒ…
            for comp_name, comp_data in result.get('comparisons', {}).items():
                logger.info(f"      â””â”€ {comp_name}: "
                          f"ç›¸å…³æ€§={comp_data['close_correlation']:.3f}, "
                          f"ä»·æ ¼å·®å¼‚={comp_data['close_mean_diff_pct']:.2f}%")
        
        logger.info("=" * 80)
        
        # ç»™å‡ºå»ºè®®
        if summary['excellent'] + summary['good'] >= total_stocks * 0.8:
            logger.info("âœ… IBæ•°æ®è´¨é‡ä¼˜ç§€ï¼Œå¯ä»¥ä½œä¸ºä¸»è¦æ•°æ®æºä½¿ç”¨")
        elif summary['excellent'] + summary['good'] + summary['fair'] >= total_stocks * 0.6:
            logger.info("âš ï¸ IBæ•°æ®è´¨é‡è‰¯å¥½ï¼Œå»ºè®®ä¸å…¶ä»–æ•°æ®æºç»“åˆä½¿ç”¨")
        else:
            logger.info("âŒ IBæ•°æ®è´¨é‡å­˜åœ¨é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥é…ç½®æˆ–ä½¿ç”¨å…¶ä»–æ•°æ®æº")


def main():
    """ä¸»å‡½æ•°"""
    try:
        validator = DataConsistencyValidator()
        
        # è¿è¡ŒéªŒè¯
        results = validator.run_full_validation()
        
        logger.info("âœ… IBæ•°æ®ä¸€è‡´æ€§éªŒè¯å®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())